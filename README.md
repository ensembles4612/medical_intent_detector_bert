# Project Overview: Medical Intent Detector Using BERT

## Project Highlights

## Project Object

To build a model using BERT that can predict what medical symptoms based on text messages. For example, the model can predict the medical intent is 'Neck pain' after parsing the text message 'There is a tingling sensation in my neck.'	



## Code and references

* **Dataset Used**: from Kaggle containing texts for common medical intents. https://www.kaggle.com/paultimothymooney/medical-speech-transcription-and-intent
* **Language**: Python 3
* **BERT Research**: from Chris McCormick https://www.chrismccormick.ai/
* **BERT Paper**: https://arxiv.org/abs/1810.04805
* **Transformers Docs**: https://huggingface.co/transformers/
* **Transformers Repo**: https://github.com/huggingface/transformers
* **Packages Used**: tensorflow, torch, numpy, pandas , seaborn, matplotlib, google.colab, sklearn, transformers
* **Colab GPU Setup**: Colab ðŸ¡’ New Notebook ðŸ¡’ Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)


## What Is BERT
The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.

## Model Building


